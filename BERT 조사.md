### 3.4 BERT

#### 3.4.1 라이선스   
Apache-2.0

#### 3.4.2 설명   
BERT는 구글에서 개발한 자연어처리(NLP) 훈련 기술이다. 특정 분야에 국한되지 않고 모든 자연어 처리 분야에서 좋은 성능을 내는 범용 Language Model이다. 
BERT는 지금까지의 자연어 처리에 활용되었던 다른 모델들보다 더 좋은 성능을 내고 있어 많은 관심을 받고 있다. 
이전에는 단어 임베딩을 위해 Word2Vec, Glove, Fasttext 방식을 사용했지만, BERT가 자연어 처리 분야의 11개 실험에서 가장 좋은 성능을 차지하면서 많이 사용되고 있다.

#### 3.4.3 기능   
* BERT는 언어 표현을 사전 학습시키는 방식을 통해 사용할 수 있다. 사전 학습은 BERT가 Wikipedia와 같은 대량의 텍스트 소스로 처음 학습되는 방법을 나타낸다.
이후 학습 결과를 [질문 답변] 및 [감정 분석]과 같은 다른 자연어 처리(NLP) 태스크에 적용할 수 있다.
BERT 및 AI Platform Training을 사용하면 약 30분 만에 다양한 NLP 모델을 학습시킬 수 있다.
* BERT를 이용하면 스팸메일 찾기, 문장 분류, 감성 분류, 두 문장 사이의 관계 분류, 문장 내 단어 라벨링, 자연어 추론, 개체명 인식, 텍스트 유사도 검사, 묻고 답히기, 등등이 가능하다.


####  3.4.3 BERT가 작동하는 과정   
Bert가 어떻게 동작하는지 자세하게 설명하자면, 크게 3단계로 진행된다.
   * 1단계로, input 이다. Bert의 input은 token,segment,position Embedding 세 가지로 이루어진다.
먼저, token은 word piece 임베딩 방식을 사용하며, 문자 단위로 임베딩 한다. Segment는 토큰 과정을 거틴 단어를 다시 하나의 문장으로 만드는 작업이다.
두 개의 문장을 구분자를 넣어 구분하고 그 두 문장을 하나의 segment로 지정하여 입력한다. Position은 토큰 순서대로 인코딩을 하는 것을 뜻한다
BERT는 위 세가지 임베딩을 합치고 이에 Layer정규화와 Dropout을 적용하여 입력으로 사용한다. 
   * 2단계는 pre-Training 이다. Bert는 사전학습을 시킨 후 사용하는 프로그램이기 때문에, 문장을 왼쪽에서 오른쪽으로 학습하여 다음 단어를 예측하는 방식을 주로 사용한다.
   * 마지막 3단계로는, Transfer Learning 단계이다. 학습된 언어모델을 전이학습시켜 실제로 자연어처리를 진행한다.
이 서비스에선 사용자에게서 받아온 키워드들을 광고 카테고리로 분류하는 방식으로 사용한다.



