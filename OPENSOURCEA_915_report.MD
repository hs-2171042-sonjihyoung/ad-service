# 시스템 설계서

### 사용자 맞춤 광고 서비스 9조-15조



### 목차

#### [1. 개요](#1.-개요)

##### [	1.1 목적](#1.1-목적)

##### [	1.2 서비스 개요](#1.2-서비스-개요)



#### [2. 유사 서비스 분석](#2.-유사-서비스-분석)

##### [	2.1 coupang ads](#2.1-coupang-ads)

[	2.1.1 기능](#2.1.1-기능)   
[	2.1.2 유사점](#2.1.2-유사점)   
[	2.1.3 차이점](#2.1.3-차이점)   

##### [	2.2 네이버 광고](#2.2-네이버-광고)

[	2.2.1 기능](#2.1.1-기능)   
[	2.2.2 유사점](#2.2.2-유사점)   
[	2.2.3 차이점](#2.2.3-차이점)   



#### [3. 오픈소스](#3.-오픈소스)

##### [	3.0 라이선스](#3.0-라이선스)

[		3.0.1 특징](#3.0.1-특징)   
[		3.0.2 사용방법](#3.0.2-사용방법)   

##### [	3.1 KOCLIP](#3.1-KOCLIP)

[		3.1.1 설명](#3.1.1-설명)   
[		3.1.2 기능](#3.1.2-기능)   
[		3.1.3 작동 과정](#3.1.3-작동-과정)   

##### [	3.2 NLTK](#3.2-NLTK)

[		3.2.1 설명](#3.2.1-설명)   
[		3.2.2 기능](#3.2.2-기능)   
[		3.2.3 작동 과정](#3.2.3-작동-과정)      
[		3.2.4 비슷한 기능을 가지는 오픈소스](#3.2.4-비슷한-기능을-가지는-오픈소스)   


##### [	3.3 Product-categories-classification](#3.3-Product-categories-classification)

[		3.3.1 설명](#3.3.1-설명)    
[		3.3.2 기능](#3.3.2-기능)   
[		3.3.3 작동 과정](#3.3.3-작동-과정)   


##### [	3.4 BERT](#3.4-BERT)
 
[		3.4.1 설명](#3.4.1-설명)   
[		3.4.2 기능](#3.4.2-기능)   
[		3.4.3 작동 과정](#3.4.3-작동-과정)   


##### [	3.5 TensorFlow](#3.5-TensorFlow)

[		3.5.1 설명](#3.5.1-설명)   
[		3.5.2 기능](#3.5.2-기능)   
[		3.5.3 작동 과정](#3.5.3-작동-과정)   


##### [	3.6 TensorFlow Extended](#3.6-TensorFlow-Extended)

[		3.6.1 설명](#3.6.1-설명)   
[		3.6.2 기능](#3.6.2-기능)   
[		3.6.3 작동 과정](#3.6.3-작동-과정)   


##### [	3.7 Geolocation API](#3.7-Geolocation-API)

[		3.7.1 설명](#3.7.1-설명)   
[		3.7.2 기능](#3.7.2-기능)   
[		3.7.3 작동 과정](#3.7.3-작동-과정)   


##### [	3.8 Places API](#3.8-Places-API)

[		3.8.1 설명](#3.8.1-설명)   
[		3.8.2 기능](#3.8.2-기능)   
[		3.8.3 작동 과정](#3.8.3-작동-과정)   



#### [4. DFD](#4.-DFD)

##### [	4.1 설명](#4.1-설명(순서대로-기재))



#### [5. GUI](#5.-Graphical-User-Interface (GUI))

##### [	5.1 Web Screen](#5.1-Web-Screen)

##### [	5.2 Mobile Screen](#5.2-Mobile-Screen)



## 1. 개요

### 1.1 목적
* 미디어의 기능이 커져가며 광고의 중요성이 높아지는 시대 속에서 더 나은 광고 환경을 제공하기 위해서 구현.

### 1.2 서비스 개요
*  광고 입찰가에 따라 제공되는 광고 방식이 아닌 사용자의 맞춤을 최우선으로 하는 광고 서비스를 목적으로 하여 사용자에게 필요하지 않는 무분별한 광고 환경을 개선한다.
 또한, 사생활 침해를 개선하기 위해서 위치를 보관하여 추적하지 않고, 현재 위치만 받아와 맞춤 광고를 제공한다.
  기존 사용자에게는 검색,이미지, 최근 3개월간의 위치 검색어 기록 데이터베이스들을 받아와 분석하여 사용자의 관심도가 높은 키워드를 기반으로 광고를 추천해주는 서비스이다.



## 2. 유사 서비스 분석

### 2.1 coupang ads

 #### 2.1.1 기능
  * 쿠팡은 사용자 검색데이터를 기반으로 알맞은 광고를 제공하며 최초 사용자에게는 이벤트성 광고나 인기 상품 위주의 광고를 제공한다.쿠팡에서 팔리는 상품 3개 중에 2개는 검색을 통해 판매된다. 쿠팡 광고는 고객 취향에 맞는 상품을 추천하고 구매하도록 도와준다. 쿠팡에서 상품광고를 집행하는 광고주 그룹은 광고 시작 후 3개월간 약 2.5배의 매출 성장을 경험하였다. 구매가 일어나는 다양한 지면(검색결과 페이지, 상품 상세 페이지, 메인 페이지 등)에 상품을 노출한다. 특정 상품의 구매 가능성이 높은 유저에게 노출하여 매출 상승을 기대할 수 있다. 키워드별 입찰가, 제안 입찰가 등 다양한 세부 기능을 통해 높은 광고 효율을 기대할 수 있다.   
매출최적화 광고를 통해 키워드/입찰가 조정 없이 자동으로 광고를 운영할 수 있다. 또한 광고주의 상황과 목적에 따라 광고 유형을 선택할 수 있다. 수익률을 고려한 안정적인 매출 성장을 원하는 광고주용 매출 최적화 광고와 상품/키워드 등 노출을 직접 컨트롤 하고 싶은 광고주용 수동 성과형 광고가 있다. 매출 최적화 광고는 목표, 광고 수익률을 직접 입력하고 입찰가, 키워드를 자동으로 최적화하여 가능한 높은 매출을 기대할 수 있으며, 수동 성과형 광고는 키워드별 별도의 입찰가 입력이 가능하고 검색/비검색 영역 별도 입찰이 가능하며 적정 제안 입찰가를 제공한다.

#### 2.1.2 유사점
 * 본 서비스와 쿠팡은 사용자 검색데이터를 기반으로 알맞은 광고를 제공하는 면에서 유사점을 가진다.

#### 2.1.3 차이점
 * 쿠팡은 최초 사용자에게 이벤트성 광고나 인기 상품 위주의 광고를 제공하는 반면에, 본 서비스는 최초 사용자의 경우 사용자의 위치를 받아오기 때문에 사용자에게 구매이력 및 검색 이력이 존재하지 않아도 본래 이벤트성 광고가 아닌 보다 사용자와 연관성 있는 광고를 추천해준다.



### 2.2 네이버 광고

#### 2.2.1 기능
 * 네이버 광고는 다양한 영역에 동시에 노출되어 더 많은 고객과 만날 수 있는 사이트검색 광고이다. 네이버 통합검색 및 네이버 내/외부의 다양한 영역에 노출되는 네이버의 대표 검색광고 상품이다. 대한민국에서 가장 많은 사람과 정보가 모이는 네이버에서 효과적으로 비즈니스를 홍보할 수 있다. 다음으로 상품을 가장 잘 보여줄 수 있는 쇼핑검색 광고이다. 광고 노출 영역을 네이버 쇼핑으로 확장한 ‘상품’단위의 이미지형 검색광고 상품이다. 구매자에게는 네이버페이 포인트 적립 혜택을 제공하여 고객 만족을 통한 광고 성과를 얻을 수 있다. 또한 좋은 콘텐츠로 소비자의 마음을 얻는 콘텐츠검색 광고이다. 양질의 콘텐츠로 검색 이용자의 정보 탐색 니즈를 충족시켜 줌으로써 신뢰 관계를 형성하고 나아가 사업주가 보유한 상품, 서비스에 대해 관심 갖고 구매할 수 있도록 가이드해 주는 콘텐츠 마케팅 상품이다. 그리고 네이버 검색결과 최상단에 노출되는 나의 브랜드, 브랜드검색이 가능하다. 최신 브랜드 콘텐츠로 고객과 소통할 수 있는 콘텐츠형 상품이다. 네이버 통합검색 결과 상단에서 다양한 템플릿을 이용하여 브랜드 아이덴티티를 표현할 수 있다. 마직막으로 지역을 기반으로 소상공인과 소비자를 효과적으로 연결하는 지역소상공인 광고이다. 가게에 위치한 주변 지역의 네이버 콘텐츠 이용자에게 스마트 플레이스에 등록한 가게 정보를 노출함으로써, 신규고객 확보 및 브랜드 인지도 강화 효과를 얻을 수 있는 배너 광고 상품이다. 

#### 2.2.2 유사점
 * 본 서비스와 네이버는 사용자가 검색했을 때, 검색한 페이지나 콘텐츠에 관련 있는 광고를 제공해주며 사용자가 있는 위치를 받아와 주변 지역 가게를 광고해준다는 면에서 유사점을 가진다.

#### 2.2.3 차이점
 * 네이버에서는 광고자의 입찰가에 따라 광고 노출 순서 및 우선순위가 결정되는 등 사용자 이외에 고려되는 조건이 있지만, 저희서비스는 사용자 이외의 정보들은 배제하여 광고를 추천해주는 사용자 기반 광고이다.



## 3. 오픈소스 


### 3.0 라이선스

#### 3.0.1 특징
1.개인적으로 쓸 수도 있고, 팔 수도 있으며, (상업적 사용 가능) 소스코드 수정도 가능하다.   
2.이 소스코드를 사용한 프로그램은 소스코드를 공개할 필요가 없다. 즉, 소스코드 필요없이 실행파일만 만들어 시중에 판매해도 된다.  
3.내가 이 프로그램을 출시 또는 나눠줄 때는 다른 라이선스를 적용할 수 있다. 
4.내가 사용한 아파치 라이선스 프로그램의 저자의 상표를 무단 사용할 수 없다.  

####  3.0.2 사용방법
* 상업적 이용, 소스코드 수정, 공개의무 없음 등의 특징들은 아래 사용 방법을 지켰을 때에 적용
되는 것이다.
1. 이 라이선스의 사본을 LICENSE 파일에 첨부해야 한다.   
2. 이 라이선스를 적용한 프로그램을 수정하여 사용한 경우 수정했다는 사실과 내용을 밝혀야 
    한다. (소스코드 공개가 아님.)   
3. 이 라이선스를 적용한 프로그램에 관련된 정보(저작권, 특허, 상표, 권리 귀속)를 NOTICE 
    파일 또는 다른 곳에 첨부해야 한다.    
4. 기존에 사용한 소스코드에 라이선스 관련 주석이 남아 있는 경우, 삭제하면 안 된다.



### 3.1 KoCLIP

* 라이선스-Apache-2.0 license

#### 3.1.1 설명

 * 코클립(KOCLIP)은 이미지가 전달되면 무작위로 샘플링된 텍스트 셋들 중에서 사용자가 검색한 이미지와 어떠한 데이터 셋이 쌍을 이루는지 분석하는 기능을 가진다.
검색한 이미지가 텍스트 데이터와 맞으면 그 텍스트를 제공하며, 이미 인터넷에서 공개적으로 사용할 수 있는 텍스트-이미지 쌍에서 학습하므로 딥러닝으로 이루어지는 사전 데이터 셋 작업들보다 비용과 인원을 절감하는 면에서 강점을 가지고 있다.

#### 3.1.2 기능

  * Image to Text : 제로 샷 이미지 분류 작업이다. 입력 이미지가 주어지면 모델은 제공된 텍스트 레이블 중에서 가장 가능성이 높은 캡션을 찾는다.
  * Text to Image : 이미지 검색 작업이다. 텍스트가 주어지면 모델은 미리 계산된 이미지 임베딩의 데이터베이스를 조회하여 주어진 텍스트와 가장 일치하는 이미지를 찾는다.
프로젝트 내에서는 이 코클립(KOCLIP)을 사용자가 검색한 이미지에 알맞은 텍스트를 추정 및 분석하여 텍스트로 반환하는 작업으로 사용했다.

#### 3.1.3 작동 과정

 * 코클립(KOCLIP)은 이미지 인코더와 텍스트 인코더를 사전 교육하여 데이터 세트의 어떤 텍스트와 어떤 이미지가 쌍을 이루는지 분석한다.
    그런 다음 코클립(KOCLIP)을 제로샷 분류기로 전환한다. 데이터 세트의 모든 클래스를 “개 사진” 같은 캡션으로 변환하고 주어진 이미지와 최상의 쌍을 추정·분석한다.

  

### 3.2 NLTK

* 라이선스-Apache-2.0 license

#### 3.2.1 설명           
 * 정의 - NLTK (Natural Language Toolkit)는 가장 널리 알려진 고성능 파이썬 NLP 라이브러리다. 주로 영어를 기반으로 하고 있다. 
이 라이브러리를 통해 자연어처리가 되면, 컴퓨터는 이 처리된 정보를 바탕으로 음성 인식, 내용 요약, 번역, 감성 분석, 텍스트 분류 작업을 할 수 있다. 텍스트를 다루기 위한 다양한 도구들을 제공한다. 분류,  토큰화,  스테밍(Stemming),  태깅,  파싱,  시멘틱 추론을 예로 들 수 있다. 

#### 3.2.2 기능                 
 * Nltk에서 지원하는 주요 기능은 말뭉치, 토큰 생성, 형태소 분석 등이 있다. 먼저, 말뭉치는 자연어 분석 작업을 위해 만든 샘플 문서 집합을 말한다. 단순히 소설, 신문 등의 문서를 모아놓은 것도 있지만 품사, 형태소 등의 보조적 의미를 추가하고 쉬운 분석을 위해 구조적인 형태로 정리해 놓은 것을 포함한다. 포함된 기능 중에서 우리가 사용할 기능인 토큰 생성은 단어를 추출하려면  nltk.tokenize 패키지를 안에 있는 word_tokenize 함수를 활용하면 문장에 있는 단어만 추출할 수 있다. 마지막으로 형태소 분석은 단어로부터 어근, 접두사, 접미사, 품사 등 다양한 언어적 속성을 파악하고 이를 이용하여 형태소를 찾아내거나 처리하는 작업이다.

#### 3.2.3 작동 과정                    
 * 우리가 사용하는 기능인 토큰 생성의 과정을 설명한다. 파이썬을 사용하여 from nltk.tokenize import word_tokenize 를 사용해주고, 단어를 나눠줄 문장을 리스트로 만들어서 넣어준다. 그 다음 list.word_tokenize를 사용해서 단어를 추출해준다. 추출한 단어는 리스트로 만들어 보여준다.

#### 3.2.4 비슷한 기능을 가지는 오픈소스                              
 * twitter-korean-text 오픈소스- https://github.com/twitter/twitter-korean-text
    이 오픈소스에서는 현재 텍스트 정규화와 형태소 분석, 스테밍을 지원하고 있다. 짧은 단어는 물론이고 긴 글도 처리할 수 있다. twitter-korean-text는 normalization, tokenization, stemming, phrase extraction 이렇게 네가지 기능을 지원한다.

  

### 3.3 Product-categories-classification

* 라이선스-Apache-2.0 license

#### 3.3.1 설명
   * Product-categories-classification은 카카오 아레나- 쇼핑몰 상품 카테고리 분류 대회에 참여해 1등의 성적을 거둔 라임로봇팀의 오픈 소스코드이다.       
      상품 카테고리 분류기로 상품의 타이틀(product 컬럼)과 이미지 특징(img_feat 컬럼)을 입력으로 활용하여 대/중/소/세 카테고리를 예측한다.     
      모델 구조의 심플함에 비해 우수한 카테고리 분류 정확도를 가진다.      

#### 3.3.2 기능
   + 상품의 타이틀(product 컬럼)과 이미지 특징(img_feat 컬럼)만 입력으로 활용한다.
   + Byte Pair Encoding(BPE) 기반의 word 분절 방법 사용한다.
   + sub-words로 분절된 word를 LSTM으로 encoding  한다.
   + class imbalance problem을 완화하기 위해 대/중/소/세에 개별 classifier 할당한다.
   + 클래스 예측 정교화- 4개 classifier의 예측된 distribution에서 가장 높은 확률값을 가지는 대/중.소 세 조합을 탐색한다.

#### 3.3.3 작동 과정
   - Getting Stared    
      - Step 1: 데이터 다운로드      
      - Step 2: 데이터 준비 및 Vocabulary 생성      
      - Step 3: 학습하기   
      - Step 4: 추론하기   

1. 광고 데이터베이스에서 받아온 데이터 다운로드한다.

2. Vocabulary 다운로드- vocab.zip 파일을 다운로드 받아 data/ 디렉터리에서 압축을 해제한다.   

3. dev.h5, text.h5 생성- python preprocess.py make_db dev, python preprocess.py make_db test   

4. pre-trained modes 다운로드- 6개 모델 묶음을 다운로드하여 output/ 디렉터리에 압축을 풀어준다.   

5. 결과파일(dev.tsv, test.tsv)생성 – python inference.py -j 12 -b 2048 –model_dir output/ --div dev, python inference.py -j 12 -b 2048 –model_Dir output/ --div test   

6. 결과파일로 BERT 언어 표현을 사전 학습시킨다.  

   

### 3.4 BERT

* 라이선스-Apache-2.0 license

#### 3.4.1 설명   
 * BERT는 구글에서 개발한 자연어처리(NLP) 훈련 기술이다. 특정 분야에 국한되지 않고 모든 자연어 처리 분야에서 좋은 성능을 내는 범용 Language Model이다. 
BERT는 지금까지의 자연어 처리에 활용되었던 다른 모델들보다 더 좋은 성능을 내고 있어 많은 관심을 받고 있다. 
이전에는 단어 임베딩을 위해 Word2Vec, Glove, Fasttext 방식을 사용했지만, BERT가 자연어 처리 분야의 11개 실험에서 가장 좋은 성능을 차지하면서 많이 사용되고 있다.

#### 3.4.2 기능   
 * BERT는 언어 표현을 사전 학습시키는 방식을 통해 사용할 수 있다. 사전 학습은 BERT가 Wikipedia와 같은 대량의 텍스트 소스로 처음 학습되는 방법을 나타낸다.
이후 학습 결과를 [질문 답변] 및 [감정 분석]과 같은 다른 자연어 처리(NLP) 태스크에 적용할 수 있다.
BERT 및 AI Platform Training을 사용하면 약 30분 만에 다양한 NLP 모델을 학습시킬 수 있다.
 * BERT를 이용하면 스팸메일 찾기, 문장 분류, 감성 분류, 두 문장 사이의 관계 분류, 문장 내 단어 라벨링, 자연어 추론, 개체명 인식, 텍스트 유사도 검사, 묻고 답히기 등등이 가능하다.


####  3.4.3 작동 과정   
 * Bert가 어떻게 동작하는지 자세하게 설명하자면, 크게 3단계로 진행된다.
    * 1단계로, input 이다. Bert의 input은 token,segment,position Embedding 세 가지로 이루어진다.
      먼저, token은 word piece 임베딩 방식을 사용하며, 문자 단위로 임베딩 한다. Segment는 토큰 과정을 거틴 단어를 다시 하나의 문장으로 만드는 작업이다.
      두 개의 문장을 구분자를 넣어 구분하고 그 두 문장을 하나의 segment로 지정하여 입력한다. Position은 토큰 순서대로 인코딩을 하는 것을 뜻한다.
      BERT는 위 세가지 임베딩을 합치고 이에 Layer정규화와 Dropout을 적용하여 입력으로 사용한다. 
    
    * 2단계는 pre-Training 이다. Bert는 사전학습을 시킨 후 사용하는 프로그램이기 때문에, 문장을 왼쪽에서 오른쪽으로 학습하여 다음 단어를 예측하는 방식을 주로 사용한다.
    
    * 마지막 3단계로는, Transfer Learning 단계이다. 학습된 언어모델을 전이학습시켜 실제로 자연어처리를 진행한다.
      이 서비스에선 사용자에게서 받아온 키워드들을 광고 카테고리로 분류하는 방식으로 사용한다.
    
      

### 3.5 TensorFlow

* 라이선스-Apache-2.0 license

#### 3.5.1 설명

* Tensor Flow는 다양한 작업에 대해 데이터 흐름 프로그래밍을 위한 오픈소스 소프트웨어 라이브러리이다. 데이터 플로우 그래프를 활용해 수치 계산을 하여, 딥 러닝과 머신 러닝 등에 활용하기 위해 개발되었다. 
* 텐서플로우는 효율적(Effiecent)이고 유연하며(Flexible), 서비스에 바로 사용할 수 있고 실무에 적합한 (Production-ready) 라이브러리라는 철학을 가지고 있다. 그리고 대용량의 데이터를 처리할 수 있도록 병렬 처리를 지원하는데 이러한 철학과 특성에 맞는 연산 구조인 데이터 플로우 그래프 연산을 사용하고 있다. 이 연산 방식을 활용해서 데이터를 풍부한 방식으로 표현하는 것도 텐서플로우의 특징이다. 

#### 3.5.2 기능

*	TensorFlow Ranking을 이용하여 가장 관련성이 높은 항목들을 순서대로 정렬해주는 역할을 한다. TensorFlow Ranking은 확장 가능한 신경망 순위 학습 (LTR) 모델을 개발하기 위한 오픈 소스 라이브러리이다.  순위 모델은 일반적으로 검색 및 추천 시스템에 사용되지만 기계 번역 , 대화 시스템 전자 상거래 , SAT 솔버 , 스마트 도시 계획 및 컴퓨터 생물학을 포함한 다양한 분야에서도 성공적으로 적용된다.
*	Tensor Flow의 주요 기능은 Python/C++을 포함한 다양한 언어 지원, 데이터 플로우 그래프를 통한 풍부한 표현력, 풍부한 리소스등이 있다

#### 3.5.3 작동 과정

* TensorFlow에서 graph는 연산을 표현해놓은 것이라서 연산을 하려면 graph가 `Session` 상에 실행되어야 한다. `Session`은 graph의 작업(op)(역자 주: operation. graph를 구성하는 노드)을 CPU나 GPU같은 `Device`에 배정하고 실행을 위한 메서드들을 제공한다. 이런 메서드들은 작업(op)을 실행해서 tensor를 만들어낸다. tensor는 파이썬에서 [numpy](http://www.numpy.org/) `ndarray` 형식으로 나오고 C 와 C++ 에서는 `TensorFlow::Tensor` 형식으로 나온다.

  

### 3.6 TensorFlow Extended 

- Apache 2.0 License


#### 3.6.1 설명

TensorFlow 기반 Google 프로덕션 규모의 ML(머신러닝) 플랫폼이다. 기계 학습 시스템의 정의, 실행 및 모니터링하는 데 필요한 공통 구성 요소를 통합하기 위한 구성 프레임워크 및 공유 라이브러리를 제공한다.

TFX를 사용하면 프로덕션 소프트웨어 배포와 모범 사례에 대한 요구사항 중 다수를 포함하는 프로덕션 ML 파이프라인을 만들 수 있다. 이는 데이터를 내부 데이터화하는 단계부터 시작해 데이터 유효성 검사, 특징 추출, 훈련, 평가, 서빙의 순서로 흘러간다.


#### 3.6.2 기능

TFX는 아래 세가지를 제공한다.

- ml pipeline과 build를 위한 toolkit
  - tfx pipeline 쓰면 airflow, apach beam, kubeflow 등에서 ml 워크플로우 조정 가능하다.
- 표준 구성 요소
  - 파이프라인의 일부 혹은 모델 학습 스크립트에 일부로 사용될 수 있는 표준 구성요소의 집합이다.
- TFX 라이브러리
  - TensorFlow Data Validation(TFDV) : 머신러닝 데이터를 분석하고 검증하기 위한 라이브러리 확장성이 뛰어나고 TensorFlow 및 TFX와 원활하게 연동되도록 설계되었다.
  - TensorFlow Transform(TFT) : TensorFlow를 사용하여 데이터를 전처리하기 위한 라이브러리이다.
  - TensorFlow : TFX를 통한 모델 학습에 사용된다. 학습 데이터 및 모델링 코드를 수집하며 SavedModel 결과를 생성. 또한 입력 데이터 사전 처리를 위해 TensorFlow Transform에서 생성한 특성 추출 파이프라인을 통합한다.
  - TensorFlow Model Analysis(TFMA) : 모델을 평가하기 위한 라이브러리. TensorFlow와 함께 사용되어 EvalSavedModel을 생성하며, EvalSavedModel은 분석의 기초가 된다. TFMA를 통해 사용자는 트레이너에 정의된 것과 동일한 측정항목을 사용하여 분산된 방식으로 대량의 데이터에서 모델을 평가한다. 이러한 측정항목은 다양한 데이터 슬라이스에 걸쳐 계산되어 Jupyter 메모장에서 시각화될 수 있다.
  - TensorFlow Metadata(TFMD) : TensorFlow를 사용하여 머신러닝 모델을 학습시킬 때 유용한 메타데이터의 표준 표현을 제공한다. 메타데이터는 입력 데이터 분석 중에 수동으로 또는 자동으로 생성될 수 있으며, 데이터 유효성 검사, 탐색 분석 및 변환에 사용된다. 메타데이터 직렬화 형식에는 아래 두가지가 포함된다.
  - ML Metadata(MLMD) : ML 개발자 및 데이터 과학자 워크플로와 관련된 메타데이터를 기록하고 검색하기 위한 라이브러리. 대체로 메타데이터는 위에서 말한 TFMD 표현을 사용한다. MLMD는 SQL-Lite, MySQL 및 기타 유사한 데이터 저장소를 사용하여 지속성을 관리한다.


#### 3.6.3 작동 과정

TensorFlow 환경에서 학습된 딥러닝 모델을 사용자에게 서빙하는 여러 가지 방법 중 TensorFlow Serving REST API를 인퍼런스 결과를 반환하는 방법을 사용한다. 이 방법은 웹 프레임워크를 이용하는 경우와 비교하여 일반적으로 처리 속도가 빠르다는 장점이 있다. 



### 3.7 Geolocation API

* 라이선스-Apache-2.0 license

#### 3.7.1 설명

* 사용자의 현재 위치 정보를 가져올 때 사용하는 자바스크립트 API이다.

#### 3.7.2 기능

* 사용자의 위도 및 경도에 관한 정보는 자바스크립트를 이용해 웹서버로 전송된다. 프로젝트 내에서 Geolocation API는 사용자의 현재 위치를 가져오는 방식으로 사용된다. 

#### 3.7.3 작동 과정

* Geolocation 요청은 POST를 사용하여 https://www.googleapis.com/geolocation/v1/geolocate?key=YOUR_API_KEY로 전송되고 요청 및 응답의 형식은 모두 JSON이다. (YOUR API KEY에는 구글 맵스 플랫폼에서 발급받은 API키를 입력한다)

  

### 3.8 Places API

* 라이선스-Apache-2.0 license

#### 3.8.1 설명

* Places API는 HTTP 요청을 사용하여 장소에 관한 정보를 반환하는 서비스이다.

#### 3.8.2 기능

* 장소검색 : 시설, 주요 관심 장소, 지리적 위치에 대한 장소 정보를 검색한다.

* 장소 세부정보 : 특정 시설 또는 관심 장소에 대한 세부정보를 가져온다.

* 장소 유형 : 장소 유형을 사용하여 장소 검색 및 자동 완성 요청의 결과를 제한한다.

#### 3.8.3 작동 과정

* HTTP 요청으로 액세스되며 JSON 또는 XML 응답을 반환해야한다. 

`json`(권장)은 출력을 JSON (JavaScript Object Notation) 형식으로 나타낸다.

`xml`는 출력을 XML로 나타낸다.

장소 서비스에 대한 모든 요청은 https:// 프로토콜을 사용하고 API 키를 포함해야한다.



# 4 DFD

<img src="https://user-images.githubusercontent.com/115949608/205011561-41231333-a84e-4fcf-a7b3-534d51c52689.PNG" width="800px" height="500px" title="px(픽셀) 크기 설정" alt="RubberDuck"></img><br/>



### 4.1 설명(순서대로 기재)
* 사용자가 구글에 접속하면 Geolocation API와 Places API는 구글 지도를 이용하여 현재 위치의 좌표값을 가져온다. 
  가져온 죄표값을 Geolocation API과 Places API에게 주면 현재위치와 주변 장소의 키워드를 얻을수 있고, 출력받은 키워드를 키워드 데이터 베이스에 전달한다.            
  
* 사용자가 구글에 접속 되었을 때 구글에서 사용자 검색어 기록 데이터 베이스, 사용자 이미지 검색어 기록 데이터 베이스, 지도의 최근 3개월 검색어 기록 데이터 베이스를 가져온다.
가져온 구글 검색어 기록 데이터 베이스는 NLTK 오픈소스를 통해 키워드로 추출되어 키워드 데이터 베이스에 전달 된다.

* 구글에서 데이터 베이스를 가져오는 과정은 검색어 기록 확인이 가능한 페이지에서 크롤링 기능을 하는 오픈소스인 scrapy를 사용해 크롤링 해온후 크롤링한 문장에서 키워드를 뽑아온다 . 또 , 데이터 사용을 허가한 사용자의 데이터들을 구글과의 계약을 통해 받아오는 등 여러 방법이 있을 수 있다.

* 사용자 이미지 검색어 기록 데이터 베이스는 KOCLIP 오픈소스에 전달되고 이미지와 일치하는 키워드로 추출되어 키워드 데이터 베이스에 전달된다.

* 지도의 검색어 기록 데이터 베이스의 데이터를 NLTK 오픈소스로 전달하고 키워드를 추출하여 키워드 데이터 베이스에 전달한다.
   * 만약 사용자가 구글에서 가져올 검색어 기록이 전혀 없는 초기 사용자이더라도 구글지도를 통해 위치정보를 가져오기 때문에 사용자 맞춤 광고를 제공할 수 있다.

* PCC는 광고 데이터 베이스에서 광고 데이터를 받아 카테고리로 분류한다. 분류된 카테고리는 BERT로 전달되어 학습된다.

* 주변 장소 키워드, 검색어 기록에서 추출된 키워드, 이미지 검색어 기록에서 추출된 키워드, 지도의 검색어 기록에서 추출된 키워드들이 있는 키워드 데이터베이스의 데이터는 BERT로 전달되어 미리 학습된 광고 카테고리로 나누어진다.

* BERT는 카테고리로 나누어진 키워드 백터를 TensorFlow로 전달한다. TensorFlow는 TensorFlow Ranking을 이용하여 우선순위 목록을 만든다. 
  tensor형태인 우선순위 목록은 TFX를 통하여 json형태로 광고 판매 플랫폼에 우선순위 목록을 전달한다.

* 우선순위 목록을 받은 광고 판매 플랫폼은 우선순위 목록에 맞춰 사용자에게 광고를 띄운다. 
이러한 방식으로 사용자 맞춤 광고를 제공한다.



## 5. Graphical User Interface (GUI)

### 5.1 Web Screen


<img src="https://user-images.githubusercontent.com/115959290/205199840-1a001073-1a27-48ac-bf10-321d627a2f85.png"></img><br/>

사용자가 일본 숙소, 호텔, 숙소 예약 등 숙소나 호텔에 관한 검색을 한 경우 위의 사진 왼쪽과 같이 그와 관련된 객실 특가 프로모션 광고를 제공한다. 또, 사용자가 옷을 이미지로 검색했던 경우 위의 사진 오른쪽과 같이 비슷한 옷 광고를 제공해준다.

### 5.2 Mobile Screen

<img src="https://user-images.githubusercontent.com/115959290/205199856-f0fc6dc0-6cae-496b-98ac-970b51d14448.png"></img><br/>
			    
검색어 기록을 가져올 수 없는 사용자는 사용자의 위치 정보를 받아 관련 광고를 제공한다. 안양시에 있는 사용자라면 사용자라면 사용자 맟춤 광고 서비슨는 안양과 관련있는 광고를 제공해준다.

모바일 화면의 경우 '환영합니다. 이기준님! 이기준님을 위한 안양동 주변 광고입니다.' 와 같이 사용자 맞춤 광고임을 나타내는 문구와 함께 광고를 내보낸다. 문구는 사용자의 이름과 맞춤 광고의 종류를 표시한다.
